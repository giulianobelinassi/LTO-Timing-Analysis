#+STARTUP: overview indent inlineimages logdrawer
#+TITLE: LTO Timing Analysis
#+AUTHOR:
#+LANGUAGE:    en
#+TAGS: noexport(n)
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

#+LATEX_CLASS_OPTIONS: [a4paper]
#+LATEX_HEADER: \usepackage[margin=1.7cm]{geometry}
#+LATEX_HEADER: \usepackage{sourcecodepro}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{array}
#+LATEX_HEADER: \usepackage{colortbl}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{algpseudocode}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage[scale=2]{ccicons}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{ragged2e}
#+LATEX_HEADER: \usepackage{textcomp}
#+LATEX_HEADER: \usepackage{pgfplots}
#+LATEX_HEADER: \usepackage{todonotes}
#+LATEX_HEADER: \renewcommand*{\UrlFont}{\ttfamily\smaller\relax}

* Parsing Data
Reading  data, converting  from wide  to long  format, saving  intermediary data
frame to =.csv/=.

#+begin_SRC julia :eval no-export :exports code
using CSV, DataFrames, DataFramesMeta

df = CSV.read("../../output_corei7.csv", delim = "; ")
df_long = stack(df, Not([:Filename, :Functions,
                         :Expected_insns, :Parallel,
                         :Inlined_percentage,
                         :Num_partitions]),
                variable_name = :Repetition,
                value_name = :CompileTime)

CSV.write("csv/corei7_long.csv", df_long)
#+end_SRC

#+RESULTS:
: "csv/corei7_long.csv"

* Looking at Data
** Histogram
Pair plot for all variables:

#+begin_SRC R :results graphics output :session *R* :file "img/histograms.png" :width 1080 :height 1080 :eval no-export :exports both :tangle "src/histograms.r"
library(ggplot2)
library(dplyr)
library(GGally)

df = read.csv("csv/corei7_long.csv")

log_df = df %>%
    select(-Repetition, -Filename) %>%
    mutate(log_Expected_insns = log(Expected_insns),
           log_Functions = log(Functions),
           log_CompileTime = log(CompileTime)) %>%
    select(log_Expected_insns,
           log_Functions,
           log_CompileTime,
           Num_partitions,
           Inlined_percentage,
           Parallel)

ggpairs(log_df)
#+end_SRC

#+RESULTS:
[[file:img/histograms.png]]

** Scatter
Scatter plots splitting by value of Parallel:

#+begin_SRC R :results graphics output :session *R* :file "img/scatter.png" :width 1080 :height 1080 :eval no-export :exports both :tangle "src/scatter.r"
library(ggplot2)
library(dplyr)
library(tidyr)

plot_df = df %>%
    pivot_longer(cols = c("Expected_insns", "Functions",
                          "Inlined_percentage", "Num_partitions"),
                 names_to = "Parameter",
                 values_to = "Value")

ggplot(plot_df) +
    facet_wrap(Parameter ~ Parallel, ncol = 4, scales = "free_x") +
    geom_point(aes(x = Value,
                   y = CompileTime,
                   color = as.factor(Parallel)),
               alpha = 0.4) +
    scale_x_log10() +
    scale_y_log10() +
    scale_color_brewer(palette = "Dark2")
#+end_SRC

#+RESULTS:
[[file:img/scatter.png]]

** Regression
Separating training  and testing data  sets, randomly picking rows.  Defining an
MSE function and fitting an arbitrary performance model.

#+begin_SRC julia :eval no-export :exports both :tangle "src/fit_model.jl"
using GLM, StatsModels, DataFrames, DataFramesMeta, Random, Statistics

df = CSV.read("csv/corei7_long.csv")

train_ratio = 0.1
train_size = round(Int, train_ratio * nrow(df))
train_rows = shuffle(1:nrow(df))[1:train_size]
df_train = df[train_rows, :]

test_rows = [x for x in 1:nrow(df) if !(x in train_rows)]
df_test = df[test_rows, :]

mse(y1, y2) = mean((y1 - y2) .^ 2)

regression_full = lm(@formula(CompileTime ~ ((Expected_insns + Expected_insns ^ 2) +
                                            (Functions + Functions ^ 2) +
                                            (Inlined_percentage + Inlined_percentage ^ 2) +
                                            (Num_partitions + Num_partitions ^ 2)) *
                                            (Parallel + Parallel ^ 2)),
                     df_train)
#+end_SRC

#+RESULTS:
#+begin_example
StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}}

CompileTime ~ 1 + Expected_insns + :(Expected_insns ^ 2) + Functions + :(Functions ^ 2) + Inlined_percentage + :(Inlined_percentage ^ 2) + Num_partitions + :(Num_partitions ^ 2) + Parallel + :(Parallel ^ 2) + Expected_insns & Parallel + Expected_insns & :(Parallel ^ 2) + :(Expected_insns ^ 2) & Parallel + :(Expected_insns ^ 2) & :(Parallel ^ 2) + Functions & Parallel + Functions & :(Parallel ^ 2) + :(Functions ^ 2) & Parallel + :(Functions ^ 2) & :(Parallel ^ 2) + Inlined_percentage & Parallel + Inlined_percentage & :(Parallel ^ 2) + :(Inlined_percentage ^ 2) & Parallel + :(Inlined_percentage ^ 2) & :(Parallel ^ 2) + Num_partitions & Parallel + Num_partitions & :(Parallel ^ 2) + :(Num_partitions ^ 2) & Parallel + :(Num_partitions ^ 2) & :(Parallel ^ 2)

Coefficients:
─────────────────────────────────────────────────────────────────────────────────────────────────────────────
                                              Coef.   Std. Error      t  Pr(>|t|)     Lower 95%     Upper 95%
─────────────────────────────────────────────────────────────────────────────────────────────────────────────
(Intercept)                             0.106783     0.0889741     1.20    0.2301  -0.0676422     0.281208
Expected_insns                          0.000136205  5.57437e-6   24.43    <1e-99   0.000125277   0.000147133
Expected_insns ^ 2                      8.66048e-11  1.80568e-11   4.80    <1e-5    5.12063e-11   1.22003e-10
Functions                               0.00622056   0.000535139  11.62    <1e-30   0.00517147    0.00726964
Functions ^ 2                          -8.53725e-7   3.67478e-7   -2.32    0.0202  -1.57413e-6   -1.3332e-7
Inlined_percentage                      1.23766      0.480655      2.57    0.0101   0.295382      2.17993
Inlined_percentage ^ 2                 -0.0370408    0.484149     -0.08    0.9390  -0.986167      0.912085
Num_partitions                         -0.00173367   0.000697614  -2.49    0.0130  -0.00310127   -0.000366066
Num_partitions ^ 2                      4.54058e-7   3.84963e-7    1.18    0.2383  -3.00624e-7    1.20874e-6
Parallel                                0.0245942    0.0539339     0.46    0.6484  -0.0811378     0.130326
Parallel ^ 2                           -0.00250601   0.00572697   -0.44    0.6617  -0.0137332     0.00872115
Expected_insns & Parallel              -2.08529e-5   3.30832e-6   -6.30    <1e-9   -2.73385e-5   -1.43673e-5
Expected_insns & Parallel ^ 2           1.27869e-6   3.52e-7       3.63    0.0003   5.88634e-7    1.96876e-6
Expected_insns ^ 2 & Parallel           1.33156e-11  1.07583e-11   1.24    0.2159  -7.77501e-12   3.44063e-11
Expected_insns ^ 2 & Parallel ^ 2      -1.19585e-12  1.14981e-12  -1.04    0.2984  -3.44994e-12   1.05824e-12
Functions & Parallel                   -0.00101894   0.000316087  -3.22    0.0013  -0.0016386    -0.000399285
Functions & Parallel ^ 2                9.24457e-5   3.33648e-5    2.77    0.0056   2.70373e-5    0.000157854
Functions ^ 2 & Parallel               -1.19456e-7   2.27848e-7   -0.52    0.6001  -5.6613e-7     3.27218e-7
Functions ^ 2 & Parallel ^ 2            1.15752e-8   2.40388e-8    0.48    0.6302  -3.55506e-8    5.8701e-8
Inlined_percentage & Parallel          -0.0176295    0.287593     -0.06    0.9511  -0.581427      0.546168
Inlined_percentage & Parallel ^ 2       0.00739879   0.0305324     0.24    0.8085  -0.052457      0.0672546
Inlined_percentage ^ 2 & Parallel      -0.124841     0.288601     -0.43    0.6653  -0.690615      0.440934
Inlined_percentage ^ 2 & Parallel ^ 2   0.00586795   0.0306436     0.19    0.8481  -0.0542058     0.0659417
Num_partitions & Parallel               0.000285324  0.000409701   0.70    0.4862  -0.000517856   0.0010885
Num_partitions & Parallel ^ 2          -2.47263e-5   4.31755e-5   -0.57    0.5669  -0.000109368   5.9915e-5
Num_partitions ^ 2 & Parallel           9.15869e-8   2.35802e-7    0.39    0.6977  -3.7068e-7     5.53854e-7
Num_partitions ^ 2 & Parallel ^ 2      -9.03051e-9   2.48468e-8   -0.36    0.7163  -5.77402e-8    3.96792e-8
─────────────────────────────────────────────────────────────────────────────────────────────────────────────
#+end_example

** MSE
Computing MSE for the trained model using the test set.

#+begin_SRC julia :eval no-export :exports both :tangle "src/fit_model.jl"
mse_full = mse(df_test[:, :CompileTime],
               predict(regression_full, select(df_test, Not(:CompileTime))))
#+end_SRC

#+RESULTS:
: 0.8054789284093229

** Cross Validation
*** Running
#+begin_SRC julia :eval no-export :exports results :tangle "src/cross_validation.jl"
using GLM, CSV, StatsModels, DataFrames, DataFramesMeta, Random, Statistics

mse(y1, y2) = mean((y1 - y2) .^ 2)

df = CSV.read("csv/corei7_long.csv")

iterations = 100

cross_validation = DataFrame(MSE = [],
                             R2 = [],
                             PredictedParallel = [],
                             RealCompileTime = [],
                             PredictedCompileTime = [])

for i = 1:iterations
    println(i)
    train_ratio = 0.1
    train_size = round(Int, train_ratio * nrow(df))
    train_rows = shuffle(1:nrow(df))[1:train_size]
    df_train = df[train_rows, :]

    test_rows = [x for x in 1:nrow(df) if !(x in train_rows)]
    df_test = df[test_rows, :]

    regression = lm(@formula(CompileTime ~ ((Expected_insns +
                                             Expected_insns ^ 2) +
                                            (Functions +
                                             Functions ^ 2) +
                                            (Inlined_percentage +
                                             Inlined_percentage ^ 2) +
                                            (Num_partitions +
                                             Num_partitions ^ 2)) *
                             (Parallel + Parallel ^ 2)), df_train)

    prediction = predict(regression, select(df_test, Not(:CompileTime)))

    best_prediction = df_test[findmin(prediction)[2], :]

    push!(cross_validation, (MSE = mse(df_test[:, :CompileTime], prediction),
                             R2 = adjr2(regression),
                             PredictedCompileTime = findmin(prediction)[1],
                             RealCompileTime = best_prediction[:CompileTime],
                             PredictedParallel = best_prediction[:Parallel]))

end

CSV.write("csv/cross_validation.csv", cross_validation)
#+end_SRC

#+RESULTS:
:RESULTS:
#+begin_example
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
#+end_example
: "csv/cross_validation.csv"
:END:
*** Computing Statistics
#+begin_SRC julia :eval no-export :exports results :tangle "src/cross_validation.jl"
cv_df = CSV.read("csv/cross_validation.csv")
cv_df
#+end_SRC

#+RESULTS:
#+begin_export html
<table class="data-frame"><thead><tr><th></th><th>MSE</th><th>R2</th><th>PredictedParallel</th><th>RealCompileTime</th><th>PredictedCompileTime</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Int64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>100 rows × 5 columns</p><tr><th>1</th><td>0.930116</td><td>0.936875</td><td>8</td><td>4.78818</td><td>-1.43669</td></tr><tr><th>2</th><td>0.844664</td><td>0.921958</td><td>1</td><td>0.0279605</td><td>0.0740601</td></tr><tr><th>3</th><td>0.773192</td><td>0.930679</td><td>1</td><td>0.0279605</td><td>0.0733869</td></tr><tr><th>4</th><td>0.772472</td><td>0.928754</td><td>4</td><td>0.0683465</td><td>0.0927624</td></tr><tr><th>5</th><td>18.213</td><td>0.918972</td><td>1</td><td>44.7125</td><td>-115.734</td></tr><tr><th>6</th><td>0.871959</td><td>0.938064</td><td>1</td><td>0.0168338</td><td>0.0754086</td></tr><tr><th>7</th><td>0.913254</td><td>0.91995</td><td>1</td><td>0.0279605</td><td>0.0683448</td></tr><tr><th>8</th><td>0.791918</td><td>0.928225</td><td>1</td><td>0.211934</td><td>0.0465965</td></tr><tr><th>9</th><td>0.862905</td><td>0.942732</td><td>1</td><td>0.0279605</td><td>0.0469905</td></tr><tr><th>10</th><td>0.859082</td><td>0.940544</td><td>1</td><td>0.0168338</td><td>0.0652974</td></tr><tr><th>11</th><td>1.02502</td><td>0.925922</td><td>1</td><td>0.0279605</td><td>0.0574469</td></tr><tr><th>12</th><td>0.992477</td><td>0.917109</td><td>1</td><td>0.0279605</td><td>0.0286201</td></tr><tr><th>13</th><td>22.8036</td><td>0.936028</td><td>8</td><td>19.9081</td><td>-244.4</td></tr><tr><th>14</th><td>0.942079</td><td>0.928529</td><td>8</td><td>19.9081</td><td>-2.52014</td></tr><tr><th>15</th><td>0.914324</td><td>0.919218</td><td>1</td><td>0.0279605</td><td>0.0447502</td></tr><tr><th>16</th><td>0.785139</td><td>0.92729</td><td>1</td><td>0.0279605</td><td>0.0670004</td></tr><tr><th>17</th><td>0.861861</td><td>0.938457</td><td>1</td><td>0.0279605</td><td>-0.00895655</td></tr><tr><th>18</th><td>1.03392</td><td>0.929546</td><td>8</td><td>19.9081</td><td>-7.12658</td></tr><tr><th>19</th><td>8.23665</td><td>0.936581</td><td>8</td><td>19.9081</td><td>-133.924</td></tr><tr><th>20</th><td>3.95871</td><td>0.931374</td><td>4</td><td>24.4301</td><td>-67.5501</td></tr><tr><th>21</th><td>11.0186</td><td>0.92417</td><td>8</td><td>19.9081</td><td>-160.604</td></tr><tr><th>22</th><td>0.789652</td><td>0.930924</td><td>1</td><td>0.0279605</td><td>0.0314176</td></tr><tr><th>23</th><td>0.825199</td><td>0.915132</td><td>1</td><td>0.211934</td><td>0.0949875</td></tr><tr><th>24</th><td>0.787516</td><td>0.930792</td><td>1</td><td>0.0279605</td><td>0.0588961</td></tr><tr><th>25</th><td>0.864626</td><td>0.926936</td><td>1</td><td>0.0279605</td><td>0.0365656</td></tr><tr><th>26</th><td>0.802192</td><td>0.93288</td><td>4</td><td>0.0683465</td><td>0.101082</td></tr><tr><th>27</th><td>0.951132</td><td>0.913545</td><td>1</td><td>0.0279605</td><td>0.0440557</td></tr><tr><th>28</th><td>0.803221</td><td>0.919849</td><td>1</td><td>0.0279605</td><td>0.0752538</td></tr><tr><th>29</th><td>0.821663</td><td>0.934066</td><td>1</td><td>0.0279605</td><td>0.0986651</td></tr><tr><th>30</th><td>0.77373</td><td>0.910285</td><td>8</td><td>0.0190939</td><td>0.101977</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>
#+end_export

Cross validated MSE, with 95% CI:

#+begin_SRC julia :eval no-export :exports results :tangle "src/cross_validation.jl"
println(mean(cv_df.MSE))
println((1.96 * std(cv_df.MSE)) / sqrt(length(cv_df.MSE)))
#+end_SRC

#+RESULTS:
: 3.3636531264181944
: 1.6902823737764627

Cross validated R2, with 95% CI:

#+begin_SRC julia :eval no-export :exports results :tangle "src/cross_validation.jl"
println(mean(cv_df.R2))
println((1.96 * std(cv_df.R2)) / sqrt(length(cv_df.R2)))
#+end_SRC

#+RESULTS:
: 0.9274544717628085
: 0.0018544508183837532
